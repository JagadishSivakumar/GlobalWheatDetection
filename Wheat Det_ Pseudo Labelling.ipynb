{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597986646756",
   "display_name": "Python 3.8.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nvidia Apex - Extension for using Pytorch\n",
    "# Url - https://github.com/NVIDIA/apex\n",
    "\n",
    "!cp -rp ../input/nvidiaapex/apex-master/* ./\n",
    "\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "timm packages\n",
    "Pytoch Image Models - with updated weights\n",
    "https://pypi.org/project/timm/\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Pycocotools\n",
    "\n",
    "Dependency for Dectectron2 \n",
    "\n",
    "COCO (Common Objects in Context) - Image dataset for object segmentation, Recognition\n",
    "\n",
    "330K Images, > 200K Labels\n",
    "\n",
    "https://cocodataset.org/#home\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\n",
    "sys.path.insert(0, \"../input/omegaconf\")\n",
    "sys.path.insert(0, \"../input/weightedboxesfusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensemble Boxes\n",
    "\n",
    "https://www.kaggle.com/c/open-images-2019-object-detection/discussion/115086\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_boxes import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from datetime import datetime\n",
    "import time\n",
    "import cv2\n",
    "import gc\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain,DetBenchEval\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from effdet.efficientdet import HeadNet\n",
    "import effdet.anchors as anchors\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from apex import amp\n",
    "import torch.nn as nn\n",
    "\n",
    "SEED = 1213\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights used\n",
    "trained_weight_path = \"best_model_path.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shonenkov Kernel of TTA and Pseudo Labels\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWheatTTA:\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    image_size = 1024\n",
    "\n",
    "    def augment(self, image):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TTAHorizontalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(1)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(2)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n",
    "        return boxes\n",
    "\n",
    "class TTAVerticalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return image.flip(2)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(3)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n",
    "        return boxes\n",
    "    \n",
    "class TTARotate90(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return torch.rot90(image, 1, (1, 2))\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return torch.rot90(images, 1, (2, 3))\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        res_boxes = boxes.copy()\n",
    "        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n",
    "        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n",
    "        return res_boxes\n",
    "\n",
    "class TTACompose(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def augment(self, image):\n",
    "        for transform in self.transforms:\n",
    "            image = transform.augment(image)\n",
    "        return image\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        for transform in self.transforms:\n",
    "            images = transform.batch_augment(images)\n",
    "        return images\n",
    "    \n",
    "    def prepare_boxes(self, boxes):\n",
    "        result_boxes = boxes.copy()\n",
    "        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n",
    "        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n",
    "        return result_boxes\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        for transform in self.transforms[::-1]:\n",
    "            boxes = transform.deaugment_boxes(boxes)\n",
    "        return self.prepare_boxes(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Augmentation - Modifications to training set\n",
    "Test Time Augmentation (TTA) - Random modifications(rotations, flipping, translations) to test images. Augmented images. Average results to get more answers\n",
    "\n",
    "https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "itertools -> product -> similar to nested for-loop\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "tta_transforms = []\n",
    "for tta_combination in product([TTAHorizontalFlip(), None], \n",
    "                               [TTAVerticalFlip(), None],\n",
    "                               [TTARotate90(), None]):\n",
    "    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tta_predictions(images, nets, score_threshold=0.25):\n",
    "    all_predictions = []\n",
    "    images = torch.stack(images).float().cuda()\n",
    "    for fold_num, net in enumerate(nets):\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            for tta_transform in tta_transforms:\n",
    "                result = []\n",
    "                det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "\n",
    "                for i in range(images.shape[0]):\n",
    "                    boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "                    scores = det[i].detach().cpu().numpy()[:,4]\n",
    "                    indexes = np.where(scores > score_threshold)[0]\n",
    "                    boxes = boxes[indexes]\n",
    "                    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "                    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "                    boxes = tta_transform.deaugment_boxes(boxes.copy())\n",
    "                    result.append({\n",
    "                        'boxes': boxes,\n",
    "                        'scores': scores[indexes],\n",
    "                    })\n",
    "                predictions.append(result)\n",
    "            all_predictions.append(predictions)\n",
    "            \n",
    "    return all_predictions\n",
    "\n",
    "def run_wbf(all_predictions, image_index, image_size=1024, iou_thr=0.44, skip_box_thr=0.43, weights=None):\n",
    "    for i in range(len(nets)):\n",
    "        predictions = all_predictions[i]\n",
    "        if i == 0:\n",
    "            boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "            scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "            labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "        else:\n",
    "            boxes += [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "            scores += [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "            labels += [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "            \n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}